-----------------------------
| Challenge 2 - Fast Malloc |
| Eddie Xie                 |
| Thomas Kaunzinger         |
-----------------------------

Results
-------

ivec: input 10000
---------------------------------
| System | HW08   | Fast Malloc |
---------------------------------
| 55ms   | 5313ms | 46ms        |
---------------------------------

list: input 1000
---------------------------------
| System | HW08   | Fast Malloc |
---------------------------------
| 24ms   | 667ms  | 7ms         |
---------------------------------

OS Information
----------------------------
Windows 10/VM Debian 10
Intel(R) Core(TM) i7-6700HQ CPU @ 2.60GHz
4 Physical Cores, 8 Logical Cores
16 GB RAM, 8169920 kB allocated to VM

Strategy and Results
--------------------
For our initial design, we tried to follow a similar approach to Facebook's jemalloc, which used a
buddy system allocator that would have unique arenas for the different threads, containing several
free lists of data at various buckets that would represent the size of each allocated node in that
level's free list. With this, we would have the benefit of not needing to scan to find available
spaces that would fit the size of the allocation. Regrettably, this proved to be very annoying to
implement, and after a few days of attempting and reaching many roadblocks, we gave up on this
approach. 

Our current approach initially started as a joke, but proved to actually be less stupid of an idea
than initially anticipated. Initially, we joked that wwe could make a really fast allocator if we
just gave a giant cache of available memory from the start, giving very fast access to the memory.
However, it turns out, that approach of working in large blocks at a time is actually not uncommon,
and is similar to the approach used by Google's own tcmalloc. In creating a large heap of space in
infrequent but large mmap calls, we can reduce the number of syscalls and amortize them to what
would be a relatively inexpensive event because of its infrequency. Of course, we needed some means
of actually dynamically adding more memory when it would become necessary, and we also needed to
make an effort to free and reintroduce memory back into the system so it wouldn't get completely
wasted. To achieve this, we could deal with the reintroduction by using a new thread for the sole
purpose of garbage collection, leaving the purpose of our free function to effectively place space
into a queue to get collected later. For our design, we had an immediately available cache stored
locally to the thread in its own free list, but when that cache grew to be large, it would instead
summon the garbage collector to coalesce that would instead add to the global heap. As for the
actual allocation, we would first take from the free list that would be in the thread's local
cache (free list and data block) but if there was none available then we would grab from the large 
global heap of memory. Only if that large space got filled would we consider calling a new mmap.
This design proved to be slightly simpler, implementation wise, than our first attempt, and ended 
up being faster than the system malloc for the provided tests.

Our allocator is significantly faster than malloc for lists because the allocation size for
lists is always the same. No colescing is done immediately, so xmalloc can just quickly
pop off a 32 byte block off the cache with no further work. We have no explanation for why
our malloc is extremely close but usually slightly faster than sysmalloc.



